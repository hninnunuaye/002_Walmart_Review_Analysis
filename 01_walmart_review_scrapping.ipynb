{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3bce9f3-7c79-4546-88a6-4d7f696f5c48",
   "metadata": {},
   "source": [
    "## Scraping of Walmart Customer Reviews (while loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def715fa-9874-4e67-b2f0-4a8bf4c3a518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.walmart.com/reviews/product/3053001212?page=1'  # Replace with desired product ID\n",
    "\n",
    "#https://httpbin.org/get\n",
    "header = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\", \"Accept-Encoding\": \"gzip, deflate, br, zstd\", \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\"}\n",
    "\n",
    "reviewlist = []\n",
    "\n",
    "\n",
    "def get_soup(url):\n",
    "  page = requests.get(url, headers=header)\n",
    "  soup = BeautifulSoup(page.text, 'html.parser')\n",
    "  return soup\n",
    "\n",
    "\n",
    "def get_reviews(soup):\n",
    "  reviews = soup.find_all('li', {'class': \"dib w-100 mb3\"})\n",
    "  print(f'Found {len(reviews)} reviews on this page.')\n",
    "  try:\n",
    "    for item in reviews:\n",
    "      review = {\n",
    "          'name': item.find('div', {'class': \"f6 gray pr2 mb2\"}).text.strip(),\n",
    "          'rating': item.find('span', {'class': \"w_iUH7\"}).text.replace(' out of 5 stars review', '').strip(),\n",
    "          'verified': True if item.find('span', {'class': \"green b mr1\"}) else False,\n",
    "          'date': item.find('div', {'class': \"f7 gray mt1\"}).string,\n",
    "          'title': item.find('h3').text.strip(),\n",
    "          'body': item.find('span', {'class': \"tl-m mb3 db-m\"}).text.strip(),\n",
    "      }\n",
    "      reviewlist.append(review)\n",
    "  except (AttributeError, KeyError):\n",
    "    pass\n",
    "\n",
    "  # Check for \"Next Page\" button and update url\n",
    "  next_page_button = soup.find('a', text='Next Page')\n",
    "  if next_page_button:\n",
    "    next_page_url = next_page_button['href']\n",
    "    # Update url for next iteration\n",
    "    url = next_page_url\n",
    "  else:\n",
    "    print('Reached last page.')\n",
    "\n",
    "\n",
    "# Loop through all review pages\n",
    "page_number = 1\n",
    "while True:\n",
    "  soup = get_soup(url)\n",
    "  print(f'Getting page: {page_number}')\n",
    "  get_reviews(soup)\n",
    "  page_number += 1\n",
    "  # Exit loop if \"Next Page\" button not found\n",
    "  if not next_page_button:\n",
    "    break\n",
    "\n",
    "# create a dataframe and export the data\n",
    "df = pd.DataFrame(reviewlist)\n",
    "df.to_excel('walmart_reviews.xlsx', index=False)\n",
    "# df.to_csv(r'/data/customer_reviews.csv', index = False)\n",
    "print('Data has been exported successfully...')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
